Master Node:
k8s-centosmaster / k8s-master (using Containerd)
3 CPU
3GB RAM
Network:
0. NAT
	IP: 10.0.2.15
	Subnet: 255.255.255.0
	GW: 10.0.2.255

1. Host Only
	IP: 192.168.56.104
	Subnet: 255.255.255.0
	GW: 192.168.56.255

2. Host Only
	IP: 192.168.56.105
	Subnet: 255.255.255.0
	GW: 192.168.56.255

Worker 1
Network:
0. NAT
	IP: 10.0.2.15
	Subnet: 255.255.255.0
	GW: 10.0.2.255

1. Host Only
	IP: 192.168.56.106
	Subnet: 255.255.255.0
	GW: 192.168.56.255


watch -n 1 kubectl get pods -A

http://mirror.centos.org/centos/7/os/x86_64/

https://zhiweiyin318.github.io/k8s-notes/kubernetes/kubeadm-v1.13.0.html

1. Install Docker (Master and Worker Node) then enable and start Docker
or Install ContainerD
	yum install -y yum-utils device-mapper-persistent-data lvm2
	yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
	yum makecache
	yum -y install containerd
	mkdir -p /etc/containerd
	containerd config default | tee /etc/containerd/config.toml
	vi /etc/containerd/config.toml
		Find the following section: [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
		And change the value of SystemdCgroup to true
	systemctl restart containerd
	ps -ef | grep containerd

wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.20.0/crictl-v1.20.0-linux-amd64.tar.gz
tar zxvf crictl-v1.20.0-linux-amd64.tar.gz -C /usr/local/bin

2. Create K8S Repo: (Master and Worker Node)
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg 
       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

3. Install kubelet, kubeadm, and kubectl (Master and Worker Node)
	sudo yum install -y kubelet-1.14.0 kubeadm-1.14.0 kubectl-1.14.0
	systemctl enable kubelet
	systemctl start kubelet

4. Set Hostname on Nodes
	sudo hostnamectl set-hostname master-node
	sudo hostnamectl set-hostname worker-node1
	sudo vi /etc/hosts
		192.168.1.10 master.k8sphils.com master-node
		192.168.1.20 node1.k8sphils.com node1 worker-node

5. Configure Firewall
	Master Node:
	sudo firewall-cmd --permanent --add-port=6443/tcp
	sudo firewall-cmd --permanent --add-port=2379-2380/tcp
	sudo firewall-cmd --permanent --add-port=10250/tcp
	sudo firewall-cmd --permanent --add-port=10251/tcp
	sudo firewall-cmd --permanent --add-port=10252/tcp
	sudo firewall-cmd --permanent --add-port=10255/tcp
	sudo firewall-cmd –-reload
	
	Worker Node:
	sudo firewall-cmd --permanent --add-port=10251/tcp
	sudo firewall-cmd --permanent --add-port=10255/tcp
	firewall-cmd –-reload

6. Update Iptables Settings (Master and Worker Nodes)
	Option 1:
	sysctl -p
	sysctl -a
	
	sysctl -a | grep net.bridge.bridge-nf-call-ip6tables
	sysctl -a | grep net.bridge.bridge-nf-call-iptables
	
	Option 2:
	vi /usr/lib/sysctl.d/00-system.conf
	net/bridge/bridge-nf-call-ip6tables = 1
	net/bridge/bridge-nf-call-iptables = 1
	net/bridge/bridge-nf-call-arptables = 1

	Option 3:
	cat  < /etc/sysctl.d/master_node_name.conf
	net.bridge.bridge-nf-call-ip6tables = 1
	net.bridge.bridge-nf-call-iptables = 1
	EOF

	Option 4:
	sysctl net.bridge.bridge-nf-call-iptables=1
	sysctl net.bridge.bridge-nf-call-ip6tables=1
	
	Then:
	sysctl --system

7. Disable SELinux (Master and Worker Node)
	sudo setenforce 0
	sudo sed -i ‘s/^SELINUX=enforcing$/SELINUX=disabled/’ /etc/selinux/config

8. Disable SWAP (Master and Worker Node)
	sudo sed -i '/swap/d' /etc/fstab
	sudo swapoff -a


**
To delete pods:
kubectl delete -n kube-system pods coredns-584795fc57-7wp44

To troubleshoot pods
kubectl describe -n kube-system pods coredns-584795fc57-7wp44

To make the master-node untainted
kubectl taint nodes --all node-role.kubernetes.io/master-
**

How to Deploy a Kubernetes Cluster:
1. Setup Pod Network (Flannel / Weave / Calico)
	kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
	
	CALICO: https://docs.projectcalico.org/v3.9/getting-started/kubernetes/installation/calico
	curl https://docs.projectcalico.org/v3.9/manifests/calico.yaml -O

	curl -O -L  https://github.com/projectcalico/calicoctl/releases/download/v3.15.1/calicoctl

	#kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml

	#kubectl apply -f https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/calicoctl.yaml
	
	CALICO_etcd:
	curl -L https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/calicoctl.yaml -o calicoctl_etcd.yaml

	CALICO_K8S API Datastore:
	curl -L https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calicoctl.yaml -o calicoctl_k8sapi.yaml

2. Create Cluster with kubeadm (Master only)
	IPADDR="192.168.56.104"
	NODENAME=$(hostname -s)
	POD_CIDR="10.244.0.0/16" - for Flannel

	sudo kubeadm init --apiserver-advertise-address=$IPADDR  --apiserver-cert-extra-sans=$IPADDR  
		--pod-network-cidr=$POD_CIDR --node-name $NODENAME --ignore-preflight-errors Swap

	Flannel:
	sudo kubeadm init --pod-network-cidr=10.244.0.0/16
	
	Calico:
	kubeadm init --pod-network-cidr=192.168.0.0/16
3. Check Status of Cluster
	sudo kubectl get pods --all-namespaces
	kube-system   coredns-6955765f44-686v8              0/1     Pending

4. Manage Cluster
	export KUBECONFIG=/etc/kubernetes/admin.conf
	
	or:
	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

5. Join Worker Node to Cluster
	Copy the Token created after the Kubeadm init: e.g.:
	kubeadm join 10.36.41.186:6443 --token czkscf.2aug6sanahffc7y9 \
    --discovery-token-ca-cert-hash sha256:13a381494346d5bf85bf06bda9f82d775aa4efb6c9f93c7df4f03c2060b49ee9
	03/09/2020:
	kubeadm join 10.36.41.149:6443 --token h1yzq6.68xcmszm0dkhnsx0 \
    --discovery-token-ca-cert-hash sha256:f58f9544e2f91183cc2b0a9cba6970c78d06612199e44a34ae3810f5a9c15f56

Test the cluster:
1. Create the namespace in which the Sock Shop will live:
	kubectl create namespace sock-shop

2. Create the actual Sock Shop application:
	kubectl apply -n sock-shop -f "https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"
	Check:
	kubectl get pods –all-namespaces
3. We’ll interact with it via the front-end service, so find the IP address for that service:
	kubectl -n sock-shop get svc front-end

	NAME        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
	front-end   10.110.250.153   <nodes>       80:30001/TCP   59s
4. Visit http://<cluster-ip> (in this case, http://10.110.250.153) with your browser


On Worker Nodes:

Documentation/kube-flannel.yml

Setting up CNI.NET.D Folder:
$ mkdir -p /etc/cni/net.d
$ cat >/etc/cni/net.d/10-mynet.conf <<EOF
{
	"cniVersion": "0.2.0",
	"name": "mynet",
	"type": "bridge",
	"bridge": "cni0",
	"isGateway": true,
	"ipMasq": true,
	"ipam": {
		"type": "host-local",
		"subnet": "10.22.0.0/16",
		"routes": [
			{ "dst": "0.0.0.0/0" }
		]
	}
}
EOF
$ cat >/etc/cni/net.d/99-loopback.conf <<EOF
{
	"cniVersion": "0.2.0",
	"name": "lo",
	"type": "loopback"
}
EOF

CALICO Configuration:
apiVersion: projectcalico.org/v3
kind: CalicoAPIConfig
metadata:
spec:
datastoreType: "etcdv3"
etcdEndpoints: "http://etcd1:2379,http://etcd2:2379"
...
